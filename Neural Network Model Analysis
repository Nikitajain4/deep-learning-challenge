1.Overview of the analysis: Explain the purpose of this analysis.
The Goal of this project is to predict and train the machine to find out whether 
applicants will be successful if funded by Alphabet Soup.



2.Data Preprocessing

I preprocessed the data by:

dropping non-beneficial columns EIN and NAME,
finding the number of data points for each unique value for each of the columns that had more
than 10 unique values - APPLICATION_TYPE and CLASSIFICATION,
choosing a cutoff point of 600 and 300, respectively, to bin rare categorical values together
into a new value called "Other",
using pd.get_dummies() to convert categorical data to numeric,
dividing the data into a target array (IS_SUCCESSFUL) and features arrays,
applying the train_test_split to create a testing and a training dataset,
and finally, using StandardScaler to scale the training and testing sets
The target variable (y) was IS_SUCCESSFUL. The data was splited into training and test subsets.


3.Compiling, Training, and Evaluating the Model

Attempt #1
layer1,units =10,activation =relu
layer2,units=20,activation=relu
layer 3,units=1,activation=sigmoid
loss:.5549,accuracy:.7302


Attempt #2
layer1,units =12,activation =relu
layer2,units=25,activation=relu
layer 3,units=1,activation=sigmoid
loss:.5562,accuracy:.7254


Attempt #3
layer1,units =10,activation =relu
layer2,units=15,activation=sigmoid
layer 3,units=1,activation=tanh
loss:.5540,accuracy:.7264


Attempt #4
layer1,units =15,activation =relu
layer2,units=30,activation=relu
layer 3,units=1,activation=tanh
loss:.5642,accuracy:.7281


I tried with different layers of units and models ,but was failed to optimize my model to 
achieve a target predive accuracy higher than 75%

